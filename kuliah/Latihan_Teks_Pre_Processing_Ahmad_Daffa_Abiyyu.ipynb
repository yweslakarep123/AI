{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 1:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
        "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
        "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
        "</ul>\n",
        "\n",
        "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
        "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n"
      ],
      "metadata": {
        "id": "npWamjVZz-0Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkOyg07Iz33z",
        "outputId": "8fbdd07a-46d2-40de-e4b6-e338e0a8cc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisasi Kalimat:\n",
            "['Halo!', 'Apa kabar?', 'Ini adalah contoh kalimat; perhatikan penggunaan enter \\ndan dash-word dalam kalimat.', \"Apakah tanda '?'\", \"atau '!'\", 'memisahkan kalimat?', \"Perhatikan pula apakah ';' memisahkan kalimat.\"]\n",
            "\n",
            "Tokenisasi Kata:\n",
            "['Halo', '!', 'Apa', 'kabar', '?', 'Ini', 'adalah', 'contoh', 'kalimat', ';', 'perhatikan', 'penggunaan', 'enter', 'dan', 'dash-word', 'dalam', 'kalimat', '.', 'Apakah', 'tanda', \"'\", '?', \"'\", 'atau', \"'\", '!', \"'\", 'memisahkan', 'kalimat', '?', 'Perhatikan', 'pula', 'apakah', \"'\", ';', \"'\", 'memisahkan', 'kalimat', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Contoh teks dengan berbagai tanda baca dan karakter khusus\n",
        "text = (\n",
        "    \"Halo! Apa kabar? Ini adalah contoh kalimat; perhatikan penggunaan enter \\n\"\n",
        "    \"dan dash-word dalam kalimat. Apakah tanda '?' atau '!' memisahkan kalimat? \"\n",
        "    \"Perhatikan pula apakah ';' memisahkan kalimat.\"\n",
        ")\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "sentences = nltk.tokenize.sent_tokenize(text)\n",
        "print(\"Tokenisasi Kalimat:\")\n",
        "print(sentences)\n",
        "\n",
        "# Tokenisasi kata\n",
        "words = nltk.tokenize.word_tokenize(text)\n",
        "print(\"\\nTokenisasi Kata:\")\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "\n",
        "<h2>Analisis Tokenisasi dengan NLTK</h2>\n",
        "\n",
        "<h3>1. Apakah tanda baca seperti \"?\" atau \"!\" akan memisahkan kalimat?</h3>\n",
        "<p>Ya, tanda baca seperti tanda tanya (<code>?</code>) dan tanda seru (<code>!</code>) dianggap sebagai pemisah kalimat oleh NLTK. Saat menggunakan <code>nltk.tokenize.sent_tokenize()</code>, setiap kalimat yang diakhiri dengan tanda ini akan dipisahkan sebagai kalimat yang berbeda.</p>\n",
        "\n",
        "<h3>2. Apakah tanda \"carriage return\"/enter/ganti baris memisahkan kalimat?</h3>\n",
        "<p>Tidak selalu. Karakter newline (<code>\\n</code>) sendiri tidak secara langsung digunakan oleh NLTK untuk memisahkan kalimat. Namun, jika ada tanda baca seperti titik (<code>.</code>), tanda seru (<code>!</code>), atau tanda tanya (<code>?</code>) sebelum newline, maka kalimat akan tetap terpisah.</p>\n",
        "\n",
        "<h3>3. Apakah \";\" memisahkan kalimat?</h3>\n",
        "<p>Tidak. Tanda titik koma (<code>;</code>) tidak dianggap sebagai pemisah kalimat dalam <code>nltk.tokenize.sent_tokenize()</code>. Sebagai hasilnya, teks sebelum dan setelah titik koma masih dianggap bagian dari satu kalimat.</p>\n",
        "\n",
        "<h3>4. Apakah tanda dash \"-\" memisahkan kata? Dalam bahasa Indonesia/Inggris?</h3>\n",
        "<p>Tidak sepenuhnya. Dalam <code>nltk.tokenize.word_tokenize()</code>, tanda dash (<code>-</code>) dalam kata majemuk seperti \"data-driven\" atau \"e-mail\" tetap dianggap sebagai bagian dari kata dan tidak memisahkan kata tersebut. Namun, jika dash digunakan dengan spasi di kedua sisi, NLTK mungkin akan menganggapnya sebagai token terpisah.</p>\n"
      ],
      "metadata": {
        "id": "PH5buUGj0meD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 2:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
        "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi Spacy juga berbeda dengan NLTK.<br />\n",
        "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
      ],
      "metadata": {
        "id": "RL5YO_wG1Smt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Analisis Tokenisasi dengan spaCy</h2>\n",
        "\n",
        "<h3>Perbandingan Hasil Tokenisasi: spaCy vs. NLTK</h3>\n",
        "<p>\n",
        "  Hasil tokenisasi antara spaCy dan NLTK tidaklah identik karena kedua library ini menggunakan pendekatan dan model yang berbeda.\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>spaCy:</strong> Menghasilkan objek <code>Doc</code> yang berisi token dengan informasi linguistik tambahan, seperti part-of-speech, dependency, dan entitas. Tokenisasi dilakukan dengan kombinasi aturan linguistik dan model statistik.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>NLTK:</strong> Menghasilkan list token (baik kata maupun kalimat) yang umumnya merupakan string sederhana. Contohnya, untuk kalimat, NLTK menggunakan model <em>Punkt</em> yang mengandalkan tanda baca sebagai pemisah.\n",
        "  </li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "kTXKHXZD1yc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W2EHH6Mv0ll4",
        "outputId": "5e811983-fca7-4cfd-87e8-f19dac5c9b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 16, in <module>\n",
            "    from .cli.info import info  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/__init__.py\", line 12, in <module>\n",
            "    from .convert import convert  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/convert.py\", line 13, in <module>\n",
            "    from ..training.converters import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/training/converters/__init__.py\", line 4, in <module>\n",
            "    from .json_to_docs import json_to_docs  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1042, in get_code\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Contoh teks dengan berbagai tanda baca dan karakter khusus\n",
        "text = (\n",
        "    \"Halo! Apa kabar? Ini adalah contoh kalimat; perhatikan penggunaan enter \\n\"\n",
        "    \"dan dash-word dalam kalimat. Apakah tanda '?' atau '!' memisahkan kalimat? \"\n",
        "    \"Perhatikan pula apakah ';' memisahkan kalimat.\"\n",
        ")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenisasi kalimat: spaCy menghasilkan objek Doc.sents yang dapat diiterasi\n",
        "sentences = list(doc.sents)\n",
        "print(\"Tokenisasi Kalimat:\")\n",
        "for sent in sentences:\n",
        "    print(sent.text)\n",
        "\n",
        "# Tokenisasi kata: menghasilkan list token berupa string\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"\\nTokenisasi Kata:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruN0Jq-A0kHN",
        "outputId": "1cb7361a-25ed-453e-f1c8-9a3587a86878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisasi Kalimat:\n",
            "Halo!\n",
            "Apa kabar?\n",
            "Ini adalah contoh kalimat; perhatikan penggunaan enter \n",
            "dan dash-word dalam kalimat.\n",
            "Apakah tanda '?' atau '!'\n",
            "memisahkan kalimat?\n",
            "Perhatikan pula apakah ';' memisahkan kalimat.\n",
            "\n",
            "Tokenisasi Kata:\n",
            "['Halo', '!', 'Apa', 'kabar', '?', 'Ini', 'adalah', 'contoh', 'kalimat', ';', 'perhatikan', 'penggunaan', 'enter', '\\n', 'dan', 'dash', '-', 'word', 'dalam', 'kalimat', '.', 'Apakah', 'tanda', \"'\", '?', \"'\", 'atau', \"'\", '!', \"'\", 'memisahkan', 'kalimat', '?', 'Perhatikan', 'pula', 'apakah', \"'\", ';', \"'\", 'memisahkan', 'kalimat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Kesimpulan</h3>\n",
        "<p>\n",
        "  Meskipun kedua library melakukan tokenisasi, hasilnya tidak sama karena:\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    Perbedaan pendekatan dalam mendeteksi batas kalimat dan kata.\n",
        "  </li>\n",
        "  <li>\n",
        "    Struktur output yang berbeda (objek <code>Doc</code> dengan metadata pada spaCy vs. list sederhana pada NLTK).\n",
        "  </li>\n",
        "</ul>\n",
        "<p>\n",
        "  Contoh ini menekankan perbedaan antara ilmu linguistik tradisional dan pendekatan komputasional linguistik modern dalam pengolahan bahasa.\n",
        "</p>"
      ],
      "metadata": {
        "id": "78f0l2Uk2G0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 3:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
        "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
      ],
      "metadata": {
        "id": "PgfFX8042KGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Analisis Tokenisasi dengan TextBlob</h2>\n",
        "\n",
        "<h3>Perbedaan Hasil Tokenisasi dengan TextBlob</h3>\n",
        "<p>\n",
        "  Hasil tokenisasi dengan TextBlob berbeda dibandingkan dengan NLTK dan spaCy karena TextBlob lebih sederhana dan lebih cocok untuk prototyping pada data kecil.\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>TextBlob:</strong> Menggunakan pendekatan berbasis NLTK tetapi menyediakan API yang lebih sederhana. Tidak sekompleks spaCy dalam analisis linguistik.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>Kelebihan:</strong> Mudah digunakan dan cocok untuk analisis cepat pada teks dalam jumlah kecil.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>Kekurangan:</strong> Tidak sekuat spaCy dalam memahami konteks kalimat dan tokenisasi yang kompleks.\n",
        "  </li>"
      ],
      "metadata": {
        "id": "ytNh4gF42cuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Contoh teks dengan berbagai tanda baca dan karakter khusus\n",
        "text = (\n",
        "    \"Halo! Apa kabar? Ini adalah contoh kalimat; perhatikan penggunaan enter \\n\"\n",
        "    \"dan dash-word dalam kalimat. Apakah tanda '?' atau '!' memisahkan kalimat? \"\n",
        "    \"Perhatikan pula apakah ';' memisahkan kalimat.\"\n",
        ")\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "sentences = blob.sentences\n",
        "print(\"Tokenisasi Kalimat:\")\n",
        "for sent in sentences:\n",
        "    print(sent)\n",
        "\n",
        "# Tokenisasi kata\n",
        "tokens = blob.words\n",
        "print(\"\\nTokenisasi Kata:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuzgkSIy2AQM",
        "outputId": "1154afa6-5793-492e-c727-2d11aec7fee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisasi Kalimat:\n",
            "Halo!\n",
            "Apa kabar?\n",
            "Ini adalah contoh kalimat; perhatikan penggunaan enter \n",
            "dan dash-word dalam kalimat.\n",
            "Apakah tanda '?'\n",
            "atau '!'\n",
            "memisahkan kalimat?\n",
            "Perhatikan pula apakah ';' memisahkan kalimat.\n",
            "\n",
            "Tokenisasi Kata:\n",
            "['Halo', 'Apa', 'kabar', 'Ini', 'adalah', 'contoh', 'kalimat', 'perhatikan', 'penggunaan', 'enter', 'dan', 'dash-word', 'dalam', 'kalimat', 'Apakah', 'tanda', 'atau', 'memisahkan', 'kalimat', 'Perhatikan', 'pula', 'apakah', 'memisahkan', 'kalimat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Kesimpulan</h3>\n",
        "<p>\n",
        "  Meskipun TextBlob mempermudah proses tokenisasi, hasilnya tidak sebaik spaCy dalam menangani kasus kompleks. Namun, untuk kebutuhan analisis cepat pada dataset kecil, TextBlob adalah pilihan yang praktis.\n",
        "</p>"
      ],
      "metadata": {
        "id": "nkS1OM1p2k2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 4:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Apakah ada perbedaan apabila menggunakan language model yang berbeda?</li>\n",
        "    <li>Bagaimana jika melakukan tokenisasi Bahasa Indonesia dengan NLTK? Apakah hasilnya sama dengan Spacy?\n",
        "</ul>\n",
        "\n"
      ],
      "metadata": {
        "id": "TUrK029s2qUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Analisis Tokenisasi Bahasa Indonesia dengan Berbagai Language Model</h2>\n",
        "\n",
        "<h3>1. Perbedaan Language Model</h3>\n",
        "<p>\n",
        "  Setiap library memiliki pendekatan dan model tersendiri dalam melakukan tokenisasi. NLTK menggunakan pre-trained <em>Punkt</em> tokenizer untuk bahasa-bahasa tertentu (misalnya, <code>english</code>, <code>french</code>, <code>german</code>, dll.). Namun, untuk bahasa Indonesia, NLTK tidak menyediakan model bawaan sehingga penggunaan parameter <code>language='indonesian'</code> akan menghasilkan error.\n",
        "</p>\n",
        "<p>\n",
        "  Di sisi lain, spaCy menyediakan model khusus untuk bahasa Indonesia (seperti <code>id_core_news_sm</code>, jika sudah terinstal) atau dapat dibuat pipeline custom dengan sentencizer. Hal ini membuat tokenisasi Bahasa Indonesia dengan spaCy cenderung lebih sesuai dengan aturan bahasa Indonesia jika model yang tepat digunakan.\n",
        "</p>\n",
        "\n",
        "<h3>2. Tokenisasi Bahasa Indonesia dengan NLTK</h3>\n",
        "<p>\n",
        "  Karena NLTK tidak memiliki model pre-trained untuk bahasa Indonesia, untuk melakukan tokenisasi kalimat pada teks Bahasa Indonesia, Anda harus melatih model sendiri menggunakan <code>PunktTrainer</code>. Data latih yang baik (korpus yang cukup besar) diperlukan agar model dapat mendeteksi batas kalimat dengan benar. Contoh di bawah ini menggunakan data contoh yang sederhana.\n",
        "</p>\n",
        "\n",
        "<h3>3. Perbandingan Hasil Tokenisasi NLTK dan spaCy</h3>\n",
        "<p>\n",
        "  Hasil tokenisasi antara NLTK dan spaCy tidak akan sama karena:\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>NLTK:</strong> Jika menggunakan model pre-trained untuk bahasa lain (misalnya, <code>english</code>), aturan yang diterapkan tidak akan sesuai untuk Bahasa Indonesia. Dengan melatih model sendiri, Anda bisa mendapatkan hasil yang lebih baik, namun kualitasnya sangat bergantung pada data latih.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>spaCy:</strong> Dengan menggunakan model Bahasa Indonesia (misalnya, <code>id_core_news_sm</code>), spaCy biasanya menghasilkan tokenisasi yang lebih akurat karena model tersebut dirancang khusus untuk bahasa tersebut.\n",
        "  </li>\n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "TpWQROvM3MXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktTrainer, PunktSentenceTokenizer\n",
        "\n",
        "# Contoh teks untuk melatih model Bahasa Indonesia (disarankan menggunakan korpus yang lebih besar)\n",
        "training_text = (\n",
        "    \"Ini adalah kalimat pertama. Ini kalimat kedua! Apakah ini kalimat ketiga? \"\n",
        "    \"Model ini dilatih dengan contoh kalimat bahasa Indonesia. \"\n",
        "    \"Tokenisasi kalimat sangat bergantung pada data latih.\"\n",
        ")\n",
        "\n",
        "# Melatih model Punkt untuk Bahasa Indonesia\n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(training_text)\n",
        "tokenizer_ind = PunktSentenceTokenizer(trainer.get_params())\n",
        "\n",
        "# Teks contoh Bahasa Indonesia\n",
        "text_id = (\n",
        "    \"Halo! Apa kabar? Ini adalah contoh kalimat dalam Bahasa Indonesia; perhatikan penggunaan enter \\n\"\n",
        "    \"dan dash-word dalam kalimat. Apakah tanda '?' atau '!' memisahkan kalimat? \"\n",
        "    \"Perhatikan pula apakah ';' memisahkan kalimat.\"\n",
        ")\n",
        "\n",
        "sentences_nltk = tokenizer_ind.tokenize(text_id)\n",
        "print(\"Tokenisasi Kalimat dengan NLTK (Bahasa Indonesia):\")\n",
        "for sent in sentences_nltk:\n",
        "    print(sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il-1pzSG2jcf",
        "outputId": "a902999c-d1dd-4ede-d9a2-c4733d750151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisasi Kalimat dengan NLTK (Bahasa Indonesia):\n",
            "Halo!\n",
            "Apa kabar?\n",
            "Ini adalah contoh kalimat dalam Bahasa Indonesia; perhatikan penggunaan enter \n",
            "dan dash-word dalam kalimat.\n",
            "Apakah tanda '?'\n",
            "atau '!'\n",
            "memisahkan kalimat?\n",
            "Perhatikan pula apakah ';' memisahkan kalimat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Mencoba memuat model Bahasa Indonesia\n",
        "try:\n",
        "    nlp_id = spacy.load(\"id_core_news_sm\")\n",
        "except Exception as e:\n",
        "    print(\"Model Bahasa Indonesia tidak ditemukan, menggunakan model blank dengan sentencizer.\")\n",
        "    nlp_id = spacy.blank(\"id\")\n",
        "    # Menambahkan komponen sentencizer untuk deteksi kalimat sederhana\n",
        "    nlp_id.add_pipe(\"sentencizer\")\n",
        "\n",
        "doc_id = nlp_id(text_id)\n",
        "\n",
        "print(\"\\nTokenisasi Kalimat dengan spaCy (Bahasa Indonesia):\")\n",
        "for sent in doc_id.sents:\n",
        "    print(sent.text)\n",
        "\n",
        "print(\"\\nTokenisasi Kata dengan spaCy (Bahasa Indonesia):\")\n",
        "tokens_spacy = [token.text for token in doc_id]\n",
        "print(tokens_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x80zMdy3SxV",
        "outputId": "b6ec9714-030a-4618-c261-6ecb4ab07831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Bahasa Indonesia tidak ditemukan, menggunakan model blank dengan sentencizer.\n",
            "\n",
            "Tokenisasi Kalimat dengan spaCy (Bahasa Indonesia):\n",
            "Halo!\n",
            "Apa kabar?\n",
            "Ini adalah contoh kalimat dalam Bahasa Indonesia; perhatikan penggunaan enter \n",
            "dan dash-word dalam kalimat.\n",
            "Apakah tanda '?'\n",
            "atau '!'\n",
            "memisahkan kalimat?\n",
            "Perhatikan pula apakah ';' memisahkan kalimat.\n",
            "\n",
            "Tokenisasi Kata dengan spaCy (Bahasa Indonesia):\n",
            "['Halo', '!', 'Apa', 'kabar', '?', 'Ini', 'adalah', 'contoh', 'kalimat', 'dalam', 'Bahasa', 'Indonesia', ';', 'perhatikan', 'penggunaan', 'enter', '\\n', 'dan', 'dash', '-', 'word', 'dalam', 'kalimat', '.', 'Apakah', 'tanda', \"'\", '?', \"'\", 'atau', \"'\", '!', \"'\", 'memisahkan', 'kalimat', '?', 'Perhatikan', 'pula', 'apakah', \"'\", ';', \"'\", 'memisahkan', 'kalimat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 5:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Temukan minimal 2 pengecualian dimana huruf besar dan kecil mempengaruhi makna dalam pemrosesan teks</li>\n",
        "    <li>Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?</li>\n",
        "    <li>Berikan contoh pengaruh dari urutan proses dalam preprocessing yang berpengaruh terhadap hasil preprocessing </li>\n",
        "        \n",
        "</ul>\n",
        "\n"
      ],
      "metadata": {
        "id": "8it8YXiI4h6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Analisis Casefolding dan Pengaruh Urutan Preprocessing</h2>\n",
        "\n",
        "<h3>1. Pengecualian di Mana Huruf Besar dan Kecil Mempengaruhi Makna</h3>\n",
        "<p>\n",
        "  Ada beberapa kasus di mana perbedaan antara huruf besar dan kecil sangat mempengaruhi makna teks, di antaranya:\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>Apple vs. apple:</strong> <em>Apple</em> (dengan huruf “A” besar) mengacu pada perusahaan teknologi ternama, sedangkan <em>apple</em> (dengan huruf kecil) umumnya merujuk pada buah apel.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>Polish vs. polish:</strong> <em>Polish</em> (huruf “P” besar) merujuk pada sesuatu yang berasal dari Polandia atau bahasa Polandia, sedangkan <em>polish</em> (huruf kecil) berarti menggosok atau membuat sesuatu menjadi berkilau.\n",
        "  </li>\n",
        "</ul>\n",
        "\n",
        "<h3>2. Efisiensi Casefolding Tanpa Tokenisasi</h3>\n",
        "<p>\n",
        "  Casefolding merupakan operasi transformasi string yang dilakukan secara karakter per karakter. Karena operasi ini hanya melibatkan konversi ke huruf kecil (atau bentuk yang distandarisasi) secara langsung, maka dapat dilakukan dengan cepat tanpa perlu melakukan tokenisasi terlebih dahulu. Dengan kata lain, casefolding tidak memerlukan pemisahan kata atau kalimat, sehingga dapat diterapkan pada seluruh teks secara langsung.\n",
        "</p>\n",
        "\n",
        "<h3>3. Pengaruh Urutan Proses dalam Preprocessing</h3>\n",
        "<p>\n",
        "  Urutan proses preprocessing dapat mempengaruhi hasil akhir. Sebagai contoh:\n",
        "</p>\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>Tokenisasi &rarr; Casefolding:</strong> Jika tokenisasi dilakukan terlebih dahulu, maka setiap token akan mempertahankan informasi aslinya (misalnya, <em>Apple</em> dan <em>apple</em> sebagai token berbeda). Setelah itu, jika dilakukan casefolding, kedua token tersebut akan berubah menjadi huruf kecil (menjadi <em>apple</em>), sehingga perbedaan makna hilang.\n",
        "  </li>\n",
        "  <li>\n",
        "    <strong>Casefolding &rarr; Tokenisasi:</strong> Jika casefolding dilakukan sebelum tokenisasi, seluruh teks sudah diubah ke bentuk huruf kecil. Proses tokenisasi selanjutnya akan bekerja pada teks yang homogen, sehingga tidak ada perbedaan antara <em>Apple</em> dan <em>apple</em>.\n",
        "  </li>\n",
        "</ul>\n",
        "<p>\n",
        "  Contoh kode berikut menunjukkan perbedaan urutan proses tersebut:\n",
        "</p>"
      ],
      "metadata": {
        "id": "DLqb1xBE5EIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh teks dengan pengecualian makna\n",
        "text = \"Apple is a tech giant, but apple can be a delicious fruit.\"\n",
        "\n",
        "# Pendekatan 1: Tokenisasi terlebih dahulu, kemudian casefolding\n",
        "tokens_before = text.split()  # Tokenisasi sederhana berdasarkan spasi\n",
        "tokens_after = [token.casefold() for token in tokens_before]\n",
        "print(\"Tokenisasi dulu, baru casefolding:\")\n",
        "print(tokens_after)\n",
        "\n",
        "# Pendekatan 2: Casefolding terlebih dahulu, kemudian tokenisasi\n",
        "lower_text = text.casefold()\n",
        "tokens_direct = lower_text.split()\n",
        "print(\"\\nCasefolding dulu, baru tokenisasi:\")\n",
        "print(tokens_direct)"
      ],
      "metadata": {
        "id": "Nt8XfqED4Wa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755374b5-0d41-4546-8506-c9a83d1b3886"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenisasi dulu, baru casefolding:\n",
            "['apple', 'is', 'a', 'tech', 'giant,', 'but', 'apple', 'can', 'be', 'a', 'delicious', 'fruit.']\n",
            "\n",
            "Casefolding dulu, baru tokenisasi:\n",
            "['apple', 'is', 'a', 'tech', 'giant,', 'but', 'apple', 'can', 'be', 'a', 'delicious', 'fruit.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p>\n",
        "  Pada kedua pendekatan tersebut, hasil akhir token akan sama-sama berupa token huruf kecil. Namun, urutan proses ini dapat mempengaruhi analisis lanjutan. Misalnya, jika ingin mempertahankan perbedaan semantik (seperti membedakan proper noun sebelum melakukan analisis entitas), sebaiknya tokenisasi dilakukan sebelum casefolding.\n",
        "</p>"
      ],
      "metadata": {
        "id": "wD-mk354dh7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 6:</font></h3>\n",
        "\n",
        "<ul>\n",
        "\t<li>Kapan harus melakukan POS tagging pada tahap preprocessing?</li>\n",
        "    <li>Buatlah contoh hasi dari POS tag dengan hanya mengambil kata yang memiliki tag NOUN (*), dan berikan contoh kasus penggunaannya?</li>\n",
        "    <li>Buatlah contoh hasil dari POS tag yang telah ditambahkan pada setiap kata dalam suatu kalimat dengan menggunakan NLTK (**)</li>\n",
        "        \n",
        "</ul>\n",
        "\n",
        "<p>(*) <strong>Input</strong>: \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"</p>\n",
        "<p>(**)</p>\n",
        "<p> <strong>Input</strong>: \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"</p>\n",
        "<p> <strong>Expected output</strong>: \"I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\"\n"
      ],
      "metadata": {
        "id": "XGwvf05sdp9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Analisis Part-of-Speech (POS) Tagging</h2>\n",
        "\n",
        "<h3>1. Kapan Harus Melakukan POS Tagging pada Tahap Preprocessing?</h3>\n",
        "<p>\n",
        "  POS tagging sebaiknya dilakukan setelah proses tokenisasi dan normalisasi (misalnya, casefolding dan stemming). Dengan demikian, setiap kata sudah teridentifikasi dengan jelas, sehingga proses tagging dapat mengassign peran gramatikal (seperti noun, verb, adjective, dll) secara akurat. Hasil POS tagging nantinya dapat mendukung analisis lebih lanjut seperti parsing, ekstraksi fitur, atau Named Entity Recognition (NER).\n",
        "</p>\n",
        "\n",
        "<h3>2. Contoh Hasil POS Tag dengan Hanya Mengambil Kata yang Memiliki Tag NOUN</h3>\n",
        "<p>\n",
        "  Pada contoh berikut, kita akan menggunakan NLTK untuk melakukan tokenisasi dan POS tagging terhadap teks berbahasa Inggris, lalu mengekstraksi kata-kata yang berfungsi sebagai NOUN. Dalam POS tagging NLTK (menggunakan tag Penn Treebank), kata benda biasanya memiliki tag yang diawali dengan <code>NN</code> (misalnya, <code>NN</code>, <code>NNS</code>, <code>NNP</code>, <code>NNPS</code>).\n",
        "</p>"
      ],
      "metadata": {
        "id": "7CV9fR0GeEml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Input teks\n",
        "text = (\"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. \"\n",
        "        \"It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. \"\n",
        "        \"It is an apex predator, primarily preying on ungulates such as deer and wild boar.\")\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Ekstraksi kata dengan tag NOUN (dimulai dengan 'NN')\n",
        "nouns = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
        "print(\"Hasil ekstraksi kata NOUN:\")\n",
        "print(nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns7tXOJ9dcl_",
        "outputId": "4aeb419e-2c4f-44bd-d22f-bdac9c258e0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil ekstraksi kata NOUN:\n",
            "['tiger', 'Panthera', 'tigris', 'cat', 'species', 'member', 'genus', 'Panthera', 'stripes', 'fur', 'underside', 'predator', 'ungulates', 'deer', 'boar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "  <em>Contoh kasus penggunaan:</em> Ekstraksi kata benda dapat digunakan dalam pembuatan kamus istilah khusus domain, analisis topik, atau sebagai fitur dalam algoritma klasifikasi teks.\n",
        "</p>\n",
        "\n",
        "<h3>3. Contoh Hasil POS Tag pada Setiap Kata dalam Suatu Kalimat (Menggunakan NLTK)</h3>\n",
        "<p>\n",
        "  Berikut adalah contoh kode yang menambahkan tag POS ke setiap kata dari sebuah kalimat, sehingga setiap token dioutput dalam format <code>kata_tag</code>.\n",
        "</p>"
      ],
      "metadata": {
        "id": "hZa-P2axeYdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input kalimat untuk POS tagging\n",
        "text2 = \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"\n",
        "\n",
        "# Tokenisasi dan POS tagging\n",
        "tokens2 = nltk.word_tokenize(text2)\n",
        "pos_tags2 = nltk.pos_tag(tokens2)\n",
        "\n",
        "# Membuat output dengan format: kata_tag\n",
        "result = \" \".join([f\"{word}_{tag}\" for word, tag in pos_tags2])\n",
        "print(\"Hasil POS tagging dengan format kata_tag:\")\n",
        "print(result)\n",
        "\n",
        "# Output yang diharapkan:\n",
        "# I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STAhQgt7eNbP",
        "outputId": "edf3e138-56fa-40e5-a5b3-fee2cc9acfcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil POS tagging dengan format kata_tag:\n",
            "I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 7:</font></h3>\n",
        "\n",
        "<p> Lakukan stopword removal pada contoh paragraf berikut ini: </p>\n",
        "<p> \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\" </p>\n"
      ],
      "metadata": {
        "id": "ScFV5815eqET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Jawaban</h1>\n",
        "<h2>Stopword Removal dengan NLTK</h2>\n",
        "\n",
        "<h3>Penjelasan</h3>\n",
        "<p>\n",
        "  Stopword removal merupakan tahap preprocessing yang penting untuk menghapus kata-kata umum (stopwords) yang tidak membawa informasi kontekstual signifikan.\n",
        "  Pada latihan ini, kita akan melakukan stopword removal pada sebuah paragraf berbahasa Indonesia menggunakan NLTK.\n",
        "</p>\n",
        "<p>\n",
        "  Karena NLTK menyediakan daftar stopwords untuk Bahasa Indonesia, kita dapat memanfaatkan <code>nltk.corpus.stopwords</code> untuk mengaksesnya.\n",
        "  Proses ini dapat dilakukan secara langsung pada teks tanpa harus melalui tokenisasi mendalam terlebih dahulu, meskipun tokenisasi dasar (misalnya, dengan <code>word_tokenize</code>) diperlukan untuk memecah teks menjadi kata-kata.\n",
        "</p>"
      ],
      "metadata": {
        "id": "iMV1fXdSe63d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Pastikan data stopwords dan punkt sudah diunduh\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Paragraf contoh\n",
        "text = (\"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. \"\n",
        "        \"Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\")\n",
        "\n",
        "# Tokenisasi teks\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Mendapatkan daftar stopwords bahasa Indonesia\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Melakukan stopword removal: hanya menyimpan token yang bukan stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Token setelah stopword removal:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji9OsEtdeeCT",
        "outputId": "d404b18c-b554-453e-c523-d754985ddea1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token setelah stopword removal:\n",
            "['Siti', 'Nurbaya', 'novel', 'Indonesia', 'ditulis', 'Marah', 'Rusli', '.', 'Novel', 'diterbitkan', 'Balai', 'Pustaka', ',', 'penerbit', 'nasional', 'negeri', 'Hindia', 'Belanda', ',', '1922', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Penjelasan Output</h3>\n",
        "<p>\n",
        "  Kode di atas akan mengeluarkan token-token yang tersisa setelah menghapus stopwords dari paragraf.\n",
        "  Proses ini membantu dalam menyederhanakan teks untuk analisis lebih lanjut dengan menghilangkan kata-kata umum yang tidak membawa banyak informasi.\n",
        "</p>"
      ],
      "metadata": {
        "id": "fpZCm5oIfMXc"
      }
    }
  ]
}