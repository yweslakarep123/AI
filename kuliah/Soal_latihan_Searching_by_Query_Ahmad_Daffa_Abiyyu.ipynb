{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IZU8UtS27LaU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D41-OsmG8AQ0",
        "outputId": "4a96800c-f77c-4413-b083-ed0b176a0cba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = 'The global average jet fuel price last week fell 1.5% compared to the week before'\n",
        "doc2 = 'Deep learning is a subset of machine learning'\n",
        "doc3 = 'Artificial intelligence and machine learning are related fields'\n",
        "doc4 = 'Neural networks are used in deep learning'\n",
        "doc5 = 'Support vector machines are a type of machine learning algorithm'\n",
        "doc6 = 'Nearly 100 years machine learning has been used in human life'\n",
        "doc7 = 'Chat GPT  is an artificial intelligence model based on Natural Language Processing'\n",
        "doc8 = 'Aviation fuels are petroleum-based fuels, or petroleum and synthetic fuel blends'"
      ],
      "metadata": {
        "id": "vzVoLuyJ8BmJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Documents\n",
        "documents = {\n",
        "    'doc1': 'The global average jet fuel price last week fell 1.5% compared to the week before',\n",
        "    'doc2': 'Deep learning is a subset of machine learning',\n",
        "    'doc3': 'Artificial intelligence and machine learning are related fields',\n",
        "    'doc4': 'Neural networks are used in deep learning',\n",
        "    'doc5': 'Support vector machines are a type of machine learning algorithm',\n",
        "    'doc6': 'Nearly 100 years machine learning has been used in human life',\n",
        "    'doc7': 'Chat GPT is an artificial intelligence model based on Natural Language Processing',\n",
        "    'doc8': 'Aviation fuels are petroleum-based fuels, or petroleum and synthetic fuel blends'\n",
        "}\n",
        "\n",
        "# Define stopwords (a basic set)\n",
        "stopwords = {'the', 'a', 'an', 'and', 'in', 'on', 'at', 'to', 'for', 'of', 'are', 'is', 'be', 'been', 'was',\n",
        "             'were', 'that', 'this', 'those', 'these', 'or', 'by', 'before', 'after', 'as', 'with', 'from'}\n",
        "\n",
        "# 1. Text preprocessing function with detailed step outputs\n",
        "def preprocess_text_detailed(text):\n",
        "    print(\"Original text:\", text)\n",
        "\n",
        "    # Case folding (convert to lowercase)\n",
        "    text_lower = text.lower()\n",
        "    print(\"After case folding:\", text_lower)\n",
        "\n",
        "    # Tokenizing (split by non-alphanumeric characters)\n",
        "    tokens = re.findall(r'\\b[a-zA-Z0-9]+\\b', text_lower)\n",
        "    print(\"After tokenizing:\", tokens)\n",
        "\n",
        "    # Stopword removal\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    print(\"After stopword removal:\", filtered_tokens)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Process each document with detailed output\n",
        "print(\"DETAILED PREPROCESSING STEPS FOR EACH DOCUMENT\")\n",
        "print(\"=\" * 80)\n",
        "preprocessed_docs = {}\n",
        "for doc_id, text in documents.items():\n",
        "    print(f\"\\nProcessing {doc_id}:\")\n",
        "    preprocessed_docs[doc_id] = preprocess_text_detailed(text)\n",
        "\n",
        "# Create vocabulary from all documents\n",
        "vocabulary = set()\n",
        "for tokens in preprocessed_docs.values():\n",
        "    vocabulary.update(tokens)\n",
        "vocabulary = sorted(list(vocabulary))\n",
        "\n",
        "print(\"\\nVOCABULARY\")\n",
        "print(\"=\" * 80)\n",
        "print(vocabulary)\n",
        "\n",
        "# Word frequency in each document\n",
        "print(\"\\nWORD FREQUENCY IN EACH DOCUMENT\")\n",
        "print(\"=\" * 80)\n",
        "word_freq = {}\n",
        "for doc_id, tokens in preprocessed_docs.items():\n",
        "    word_freq[doc_id] = Counter(tokens)\n",
        "    print(f\"{doc_id}: {dict(word_freq[doc_id])}\")\n",
        "\n",
        "# Create binary matrix (word appears or not)\n",
        "binary_matrix = pd.DataFrame(0, index=list(documents.keys()), columns=vocabulary)\n",
        "for doc_id, tokens in preprocessed_docs.items():\n",
        "    for token in set(tokens):  # Using set to count each unique token only once\n",
        "        binary_matrix.loc[doc_id, token] = 1\n",
        "\n",
        "print(\"\\nBINARY MATRIX (WORD PRESENCE)\")\n",
        "print(\"=\" * 80)\n",
        "print(binary_matrix)\n",
        "\n",
        "# Calculate Term Frequency (TF)\n",
        "tf_matrix = pd.DataFrame(0.0, index=list(documents.keys()), columns=vocabulary)\n",
        "for doc_id, counter in word_freq.items():\n",
        "    for word, count in counter.items():\n",
        "        # Normalized TF (divide by document length)\n",
        "        tf_matrix.loc[doc_id, word] = count / len(preprocessed_docs[doc_id])\n",
        "\n",
        "print(\"\\nTERM FREQUENCY (TF) MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "print(tf_matrix)\n",
        "\n",
        "# Calculate Document Frequency (DF)\n",
        "df_values = binary_matrix.sum(axis=0)\n",
        "\n",
        "print(\"\\nDOCUMENT FREQUENCY (DF)\")\n",
        "print(\"=\" * 80)\n",
        "print(df_values)\n",
        "\n",
        "# Calculate Inverse Document Frequency (IDF) using 1+log formula\n",
        "num_docs = len(documents)\n",
        "idf_values = pd.Series(index=vocabulary)\n",
        "for term in vocabulary:\n",
        "    if df_values[term] > 0:\n",
        "        idf_values[term] = 1 + np.log10(num_docs / df_values[term])\n",
        "    else:\n",
        "        idf_values[term] = 1  # For terms that don't appear in any document (shouldn't happen)\n",
        "\n",
        "print(\"\\nINVERSE DOCUMENT FREQUENCY (IDF) USING 1+log10(N/df)\")\n",
        "print(\"=\" * 80)\n",
        "print(idf_values)\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tfidf_matrix = tf_matrix.copy()\n",
        "for term in vocabulary:\n",
        "    tfidf_matrix[term] = tf_matrix[term] * idf_values[term]\n",
        "\n",
        "print(\"\\nTF-IDF MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "print(tfidf_matrix)\n",
        "\n",
        "# Process query for Rocchio\n",
        "query = \"machine learning\"\n",
        "print(\"\\nQUERY PREPROCESSING\")\n",
        "print(\"=\" * 80)\n",
        "preprocessed_query = preprocess_text_detailed(query)\n",
        "\n",
        "# Create query vector\n",
        "query_vector = pd.Series(0.0, index=vocabulary)\n",
        "for term in preprocessed_query:\n",
        "    if term in vocabulary:\n",
        "        query_vector[term] = 1.0\n",
        "\n",
        "print(\"\\nORIGINAL QUERY VECTOR\")\n",
        "print(\"=\" * 80)\n",
        "print(query_vector)\n",
        "\n",
        "# Calculate relevancy score using cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# Calculate initial relevance scores\n",
        "initial_scores = {}\n",
        "for doc_id in documents.keys():\n",
        "    initial_scores[doc_id] = cosine_similarity(query_vector, tfidf_matrix.loc[doc_id])\n",
        "\n",
        "print(\"\\nINITIAL RELEVANCE SCORES (COSINE SIMILARITY)\")\n",
        "print(\"=\" * 80)\n",
        "for doc_id, score in sorted(initial_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{doc_id}: {score:.4f}\")\n",
        "\n",
        "# Sort documents by initial score\n",
        "sorted_docs = sorted(initial_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Assuming top 3 documents are relevant, rest are irrelevant\n",
        "relevant_docs = [doc_id for doc_id, _ in sorted_docs[:3]]\n",
        "irrelevant_docs = [doc_id for doc_id, _ in sorted_docs[3:]]\n",
        "\n",
        "print(\"\\nRELEVANT DOCUMENTS FOR QUERY EXPANSION\")\n",
        "print(\"=\" * 80)\n",
        "print(relevant_docs)\n",
        "\n",
        "print(\"\\nIRRELEVANT DOCUMENTS FOR QUERY EXPANSION\")\n",
        "print(\"=\" * 80)\n",
        "print(irrelevant_docs)\n",
        "\n",
        "# Implement Rocchio algorithm with query expansion according to lecture\n",
        "def rocchio_algorithm(original_query, relevant_docs, irrelevant_docs, alpha=1.0, beta=0.75, gamma=0.15):\n",
        "    \"\"\"\n",
        "    Rocchio algorithm for query expansion based on lecture material\n",
        "\n",
        "    Parameters:\n",
        "    original_query: Original query vector\n",
        "    relevant_docs: List of relevant document IDs\n",
        "    irrelevant_docs: List of non-relevant document IDs\n",
        "    alpha: Weight for original query (default 1.0)\n",
        "    beta: Weight for relevant documents (default 0.75)\n",
        "    gamma: Weight for non-relevant documents (default 0.15)\n",
        "\n",
        "    Returns:\n",
        "    Modified query vector\n",
        "    \"\"\"\n",
        "    # Start with weighted original query\n",
        "    modified_query = alpha * original_query\n",
        "\n",
        "    # Add contribution from relevant documents (centroid of relevant docs)\n",
        "    if relevant_docs:\n",
        "        relevant_centroid = tfidf_matrix.loc[relevant_docs].mean(axis=0)\n",
        "        modified_query += beta * relevant_centroid\n",
        "\n",
        "    # Subtract contribution from irrelevant documents (centroid of non-relevant docs)\n",
        "    if irrelevant_docs:\n",
        "        irrelevant_centroid = tfidf_matrix.loc[irrelevant_docs].mean(axis=0)\n",
        "        modified_query -= gamma * irrelevant_centroid\n",
        "\n",
        "    # Set negative weights to 0 as mentioned in the lecture\n",
        "    modified_query = modified_query.clip(lower=0)\n",
        "\n",
        "    return modified_query\n",
        "\n",
        "# Apply Rocchio algorithm to expand the query\n",
        "# Using values from the lecture: α = 1.0, β = 0.75, γ = 0.15\n",
        "expanded_query = rocchio_algorithm(query_vector, relevant_docs, irrelevant_docs)\n",
        "\n",
        "print(\"\\nEXPANDED QUERY VECTOR (AFTER ROCCHIO)\")\n",
        "print(\"=\" * 80)\n",
        "print(expanded_query)\n",
        "\n",
        "# Show the top 10 terms in the expanded query\n",
        "expanded_terms = expanded_query.sort_values(ascending=False).head(10)\n",
        "print(\"\\nTOP 10 TERMS IN EXPANDED QUERY\")\n",
        "print(\"=\" * 80)\n",
        "for term, weight in expanded_terms.items():\n",
        "    if weight > 0:\n",
        "        print(f\"{term}: {weight:.4f}\")\n",
        "\n",
        "# Calculate final relevance scores with expanded query\n",
        "final_scores = {}\n",
        "for doc_id in documents.keys():\n",
        "    final_scores[doc_id] = cosine_similarity(expanded_query, tfidf_matrix.loc[doc_id])\n",
        "\n",
        "print(\"\\nFINAL RELEVANCE SCORES WITH ROCCHIO EXPANSION\")\n",
        "print(\"=\" * 80)\n",
        "for doc_id, score in sorted(final_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{doc_id}: {score:.4f}\")\n",
        "\n",
        "# Show the difference in rankings\n",
        "print(\"\\nCOMPARISON OF RANKINGS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Original Ranking vs. Expanded Query Ranking\")\n",
        "original_ranking = [doc_id for doc_id, _ in sorted(initial_scores.items(), key=lambda x: x[1], reverse=True)]\n",
        "expanded_ranking = [doc_id for doc_id, _ in sorted(final_scores.items(), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "for i in range(len(documents)):\n",
        "    print(f\"Rank {i+1}: {original_ranking[i]} → {expanded_ranking[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHddIME_8txR",
        "outputId": "93afc145-ff4a-41d7-cdfb-200de241e3d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DETAILED PREPROCESSING STEPS FOR EACH DOCUMENT\n",
            "================================================================================\n",
            "\n",
            "Processing doc1:\n",
            "Original text: The global average jet fuel price last week fell 1.5% compared to the week before\n",
            "After case folding: the global average jet fuel price last week fell 1.5% compared to the week before\n",
            "After tokenizing: ['the', 'global', 'average', 'jet', 'fuel', 'price', 'last', 'week', 'fell', '1', '5', 'compared', 'to', 'the', 'week', 'before']\n",
            "After stopword removal: ['global', 'average', 'jet', 'fuel', 'price', 'last', 'week', 'fell', '1', '5', 'compared', 'week']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc2:\n",
            "Original text: Deep learning is a subset of machine learning\n",
            "After case folding: deep learning is a subset of machine learning\n",
            "After tokenizing: ['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning']\n",
            "After stopword removal: ['deep', 'learning', 'subset', 'machine', 'learning']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc3:\n",
            "Original text: Artificial intelligence and machine learning are related fields\n",
            "After case folding: artificial intelligence and machine learning are related fields\n",
            "After tokenizing: ['artificial', 'intelligence', 'and', 'machine', 'learning', 'are', 'related', 'fields']\n",
            "After stopword removal: ['artificial', 'intelligence', 'machine', 'learning', 'related', 'fields']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc4:\n",
            "Original text: Neural networks are used in deep learning\n",
            "After case folding: neural networks are used in deep learning\n",
            "After tokenizing: ['neural', 'networks', 'are', 'used', 'in', 'deep', 'learning']\n",
            "After stopword removal: ['neural', 'networks', 'used', 'deep', 'learning']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc5:\n",
            "Original text: Support vector machines are a type of machine learning algorithm\n",
            "After case folding: support vector machines are a type of machine learning algorithm\n",
            "After tokenizing: ['support', 'vector', 'machines', 'are', 'a', 'type', 'of', 'machine', 'learning', 'algorithm']\n",
            "After stopword removal: ['support', 'vector', 'machines', 'type', 'machine', 'learning', 'algorithm']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc6:\n",
            "Original text: Nearly 100 years machine learning has been used in human life\n",
            "After case folding: nearly 100 years machine learning has been used in human life\n",
            "After tokenizing: ['nearly', '100', 'years', 'machine', 'learning', 'has', 'been', 'used', 'in', 'human', 'life']\n",
            "After stopword removal: ['nearly', '100', 'years', 'machine', 'learning', 'has', 'used', 'human', 'life']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc7:\n",
            "Original text: Chat GPT is an artificial intelligence model based on Natural Language Processing\n",
            "After case folding: chat gpt is an artificial intelligence model based on natural language processing\n",
            "After tokenizing: ['chat', 'gpt', 'is', 'an', 'artificial', 'intelligence', 'model', 'based', 'on', 'natural', 'language', 'processing']\n",
            "After stopword removal: ['chat', 'gpt', 'artificial', 'intelligence', 'model', 'based', 'natural', 'language', 'processing']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Processing doc8:\n",
            "Original text: Aviation fuels are petroleum-based fuels, or petroleum and synthetic fuel blends\n",
            "After case folding: aviation fuels are petroleum-based fuels, or petroleum and synthetic fuel blends\n",
            "After tokenizing: ['aviation', 'fuels', 'are', 'petroleum', 'based', 'fuels', 'or', 'petroleum', 'and', 'synthetic', 'fuel', 'blends']\n",
            "After stopword removal: ['aviation', 'fuels', 'petroleum', 'based', 'fuels', 'petroleum', 'synthetic', 'fuel', 'blends']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "VOCABULARY\n",
            "================================================================================\n",
            "['1', '100', '5', 'algorithm', 'artificial', 'average', 'aviation', 'based', 'blends', 'chat', 'compared', 'deep', 'fell', 'fields', 'fuel', 'fuels', 'global', 'gpt', 'has', 'human', 'intelligence', 'jet', 'language', 'last', 'learning', 'life', 'machine', 'machines', 'model', 'natural', 'nearly', 'networks', 'neural', 'petroleum', 'price', 'processing', 'related', 'subset', 'support', 'synthetic', 'type', 'used', 'vector', 'week', 'years']\n",
            "\n",
            "WORD FREQUENCY IN EACH DOCUMENT\n",
            "================================================================================\n",
            "doc1: {'global': 1, 'average': 1, 'jet': 1, 'fuel': 1, 'price': 1, 'last': 1, 'week': 2, 'fell': 1, '1': 1, '5': 1, 'compared': 1}\n",
            "doc2: {'deep': 1, 'learning': 2, 'subset': 1, 'machine': 1}\n",
            "doc3: {'artificial': 1, 'intelligence': 1, 'machine': 1, 'learning': 1, 'related': 1, 'fields': 1}\n",
            "doc4: {'neural': 1, 'networks': 1, 'used': 1, 'deep': 1, 'learning': 1}\n",
            "doc5: {'support': 1, 'vector': 1, 'machines': 1, 'type': 1, 'machine': 1, 'learning': 1, 'algorithm': 1}\n",
            "doc6: {'nearly': 1, '100': 1, 'years': 1, 'machine': 1, 'learning': 1, 'has': 1, 'used': 1, 'human': 1, 'life': 1}\n",
            "doc7: {'chat': 1, 'gpt': 1, 'artificial': 1, 'intelligence': 1, 'model': 1, 'based': 1, 'natural': 1, 'language': 1, 'processing': 1}\n",
            "doc8: {'aviation': 1, 'fuels': 2, 'petroleum': 2, 'based': 1, 'synthetic': 1, 'fuel': 1, 'blends': 1}\n",
            "\n",
            "BINARY MATRIX (WORD PRESENCE)\n",
            "================================================================================\n",
            "      1  100  5  algorithm  artificial  average  aviation  based  blends  \\\n",
            "doc1  1    0  1          0           0        1         0      0       0   \n",
            "doc2  0    0  0          0           0        0         0      0       0   \n",
            "doc3  0    0  0          0           1        0         0      0       0   \n",
            "doc4  0    0  0          0           0        0         0      0       0   \n",
            "doc5  0    0  0          1           0        0         0      0       0   \n",
            "doc6  0    1  0          0           0        0         0      0       0   \n",
            "doc7  0    0  0          0           1        0         0      1       0   \n",
            "doc8  0    0  0          0           0        0         1      1       1   \n",
            "\n",
            "      chat  ...  processing  related  subset  support  synthetic  type  used  \\\n",
            "doc1     0  ...           0        0       0        0          0     0     0   \n",
            "doc2     0  ...           0        0       1        0          0     0     0   \n",
            "doc3     0  ...           0        1       0        0          0     0     0   \n",
            "doc4     0  ...           0        0       0        0          0     0     1   \n",
            "doc5     0  ...           0        0       0        1          0     1     0   \n",
            "doc6     0  ...           0        0       0        0          0     0     1   \n",
            "doc7     1  ...           1        0       0        0          0     0     0   \n",
            "doc8     0  ...           0        0       0        0          1     0     0   \n",
            "\n",
            "      vector  week  years  \n",
            "doc1       0     1      0  \n",
            "doc2       0     0      0  \n",
            "doc3       0     0      0  \n",
            "doc4       0     0      0  \n",
            "doc5       1     0      0  \n",
            "doc6       0     0      1  \n",
            "doc7       0     0      0  \n",
            "doc8       0     0      0  \n",
            "\n",
            "[8 rows x 45 columns]\n",
            "\n",
            "TERM FREQUENCY (TF) MATRIX\n",
            "================================================================================\n",
            "             1       100         5  algorithm  artificial   average  aviation  \\\n",
            "doc1  0.083333  0.000000  0.083333   0.000000    0.000000  0.083333  0.000000   \n",
            "doc2  0.000000  0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
            "doc3  0.000000  0.000000  0.000000   0.000000    0.166667  0.000000  0.000000   \n",
            "doc4  0.000000  0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
            "doc5  0.000000  0.000000  0.000000   0.142857    0.000000  0.000000  0.000000   \n",
            "doc6  0.000000  0.111111  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
            "doc7  0.000000  0.000000  0.000000   0.000000    0.111111  0.000000  0.000000   \n",
            "doc8  0.000000  0.000000  0.000000   0.000000    0.000000  0.000000  0.111111   \n",
            "\n",
            "         based    blends      chat  ...  processing   related  subset  \\\n",
            "doc1  0.000000  0.000000  0.000000  ...    0.000000  0.000000     0.0   \n",
            "doc2  0.000000  0.000000  0.000000  ...    0.000000  0.000000     0.2   \n",
            "doc3  0.000000  0.000000  0.000000  ...    0.000000  0.166667     0.0   \n",
            "doc4  0.000000  0.000000  0.000000  ...    0.000000  0.000000     0.0   \n",
            "doc5  0.000000  0.000000  0.000000  ...    0.000000  0.000000     0.0   \n",
            "doc6  0.000000  0.000000  0.000000  ...    0.000000  0.000000     0.0   \n",
            "doc7  0.111111  0.000000  0.111111  ...    0.111111  0.000000     0.0   \n",
            "doc8  0.111111  0.111111  0.000000  ...    0.000000  0.000000     0.0   \n",
            "\n",
            "       support  synthetic      type      used    vector      week     years  \n",
            "doc1  0.000000   0.000000  0.000000  0.000000  0.000000  0.166667  0.000000  \n",
            "doc2  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "doc3  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "doc4  0.000000   0.000000  0.000000  0.200000  0.000000  0.000000  0.000000  \n",
            "doc5  0.142857   0.000000  0.142857  0.000000  0.142857  0.000000  0.000000  \n",
            "doc6  0.000000   0.000000  0.000000  0.111111  0.000000  0.000000  0.111111  \n",
            "doc7  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "doc8  0.000000   0.111111  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[8 rows x 45 columns]\n",
            "\n",
            "DOCUMENT FREQUENCY (DF)\n",
            "================================================================================\n",
            "1               1\n",
            "100             1\n",
            "5               1\n",
            "algorithm       1\n",
            "artificial      2\n",
            "average         1\n",
            "aviation        1\n",
            "based           2\n",
            "blends          1\n",
            "chat            1\n",
            "compared        1\n",
            "deep            2\n",
            "fell            1\n",
            "fields          1\n",
            "fuel            2\n",
            "fuels           1\n",
            "global          1\n",
            "gpt             1\n",
            "has             1\n",
            "human           1\n",
            "intelligence    2\n",
            "jet             1\n",
            "language        1\n",
            "last            1\n",
            "learning        5\n",
            "life            1\n",
            "machine         4\n",
            "machines        1\n",
            "model           1\n",
            "natural         1\n",
            "nearly          1\n",
            "networks        1\n",
            "neural          1\n",
            "petroleum       1\n",
            "price           1\n",
            "processing      1\n",
            "related         1\n",
            "subset          1\n",
            "support         1\n",
            "synthetic       1\n",
            "type            1\n",
            "used            2\n",
            "vector          1\n",
            "week            1\n",
            "years           1\n",
            "dtype: int64\n",
            "\n",
            "INVERSE DOCUMENT FREQUENCY (IDF) USING 1+log10(N/df)\n",
            "================================================================================\n",
            "1               1.90309\n",
            "100             1.90309\n",
            "5               1.90309\n",
            "algorithm       1.90309\n",
            "artificial      1.60206\n",
            "average         1.90309\n",
            "aviation        1.90309\n",
            "based           1.60206\n",
            "blends          1.90309\n",
            "chat            1.90309\n",
            "compared        1.90309\n",
            "deep            1.60206\n",
            "fell            1.90309\n",
            "fields          1.90309\n",
            "fuel            1.60206\n",
            "fuels           1.90309\n",
            "global          1.90309\n",
            "gpt             1.90309\n",
            "has             1.90309\n",
            "human           1.90309\n",
            "intelligence    1.60206\n",
            "jet             1.90309\n",
            "language        1.90309\n",
            "last            1.90309\n",
            "learning        1.20412\n",
            "life            1.90309\n",
            "machine         1.30103\n",
            "machines        1.90309\n",
            "model           1.90309\n",
            "natural         1.90309\n",
            "nearly          1.90309\n",
            "networks        1.90309\n",
            "neural          1.90309\n",
            "petroleum       1.90309\n",
            "price           1.90309\n",
            "processing      1.90309\n",
            "related         1.90309\n",
            "subset          1.90309\n",
            "support         1.90309\n",
            "synthetic       1.90309\n",
            "type            1.90309\n",
            "used            1.60206\n",
            "vector          1.90309\n",
            "week            1.90309\n",
            "years           1.90309\n",
            "dtype: float64\n",
            "\n",
            "TF-IDF MATRIX\n",
            "================================================================================\n",
            "             1       100         5  algorithm  artificial   average  aviation  \\\n",
            "doc1  0.158591  0.000000  0.158591    0.00000    0.000000  0.158591  0.000000   \n",
            "doc2  0.000000  0.000000  0.000000    0.00000    0.000000  0.000000  0.000000   \n",
            "doc3  0.000000  0.000000  0.000000    0.00000    0.267010  0.000000  0.000000   \n",
            "doc4  0.000000  0.000000  0.000000    0.00000    0.000000  0.000000  0.000000   \n",
            "doc5  0.000000  0.000000  0.000000    0.27187    0.000000  0.000000  0.000000   \n",
            "doc6  0.000000  0.211454  0.000000    0.00000    0.000000  0.000000  0.000000   \n",
            "doc7  0.000000  0.000000  0.000000    0.00000    0.178007  0.000000  0.000000   \n",
            "doc8  0.000000  0.000000  0.000000    0.00000    0.000000  0.000000  0.211454   \n",
            "\n",
            "         based    blends      chat  ...  processing   related    subset  \\\n",
            "doc1  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
            "doc2  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.380618   \n",
            "doc3  0.000000  0.000000  0.000000  ...    0.000000  0.317182  0.000000   \n",
            "doc4  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
            "doc5  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
            "doc6  0.000000  0.000000  0.000000  ...    0.000000  0.000000  0.000000   \n",
            "doc7  0.178007  0.000000  0.211454  ...    0.211454  0.000000  0.000000   \n",
            "doc8  0.178007  0.211454  0.000000  ...    0.000000  0.000000  0.000000   \n",
            "\n",
            "      support  synthetic     type      used   vector      week     years  \n",
            "doc1  0.00000   0.000000  0.00000  0.000000  0.00000  0.317182  0.000000  \n",
            "doc2  0.00000   0.000000  0.00000  0.000000  0.00000  0.000000  0.000000  \n",
            "doc3  0.00000   0.000000  0.00000  0.000000  0.00000  0.000000  0.000000  \n",
            "doc4  0.00000   0.000000  0.00000  0.320412  0.00000  0.000000  0.000000  \n",
            "doc5  0.27187   0.000000  0.27187  0.000000  0.27187  0.000000  0.000000  \n",
            "doc6  0.00000   0.000000  0.00000  0.178007  0.00000  0.000000  0.211454  \n",
            "doc7  0.00000   0.000000  0.00000  0.000000  0.00000  0.000000  0.000000  \n",
            "doc8  0.00000   0.211454  0.00000  0.000000  0.00000  0.000000  0.000000  \n",
            "\n",
            "[8 rows x 45 columns]\n",
            "\n",
            "QUERY PREPROCESSING\n",
            "================================================================================\n",
            "Original text: machine learning\n",
            "After case folding: machine learning\n",
            "After tokenizing: ['machine', 'learning']\n",
            "After stopword removal: ['machine', 'learning']\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ORIGINAL QUERY VECTOR\n",
            "================================================================================\n",
            "1               0.0\n",
            "100             0.0\n",
            "5               0.0\n",
            "algorithm       0.0\n",
            "artificial      0.0\n",
            "average         0.0\n",
            "aviation        0.0\n",
            "based           0.0\n",
            "blends          0.0\n",
            "chat            0.0\n",
            "compared        0.0\n",
            "deep            0.0\n",
            "fell            0.0\n",
            "fields          0.0\n",
            "fuel            0.0\n",
            "fuels           0.0\n",
            "global          0.0\n",
            "gpt             0.0\n",
            "has             0.0\n",
            "human           0.0\n",
            "intelligence    0.0\n",
            "jet             0.0\n",
            "language        0.0\n",
            "last            0.0\n",
            "learning        1.0\n",
            "life            0.0\n",
            "machine         1.0\n",
            "machines        0.0\n",
            "model           0.0\n",
            "natural         0.0\n",
            "nearly          0.0\n",
            "networks        0.0\n",
            "neural          0.0\n",
            "petroleum       0.0\n",
            "price           0.0\n",
            "processing      0.0\n",
            "related         0.0\n",
            "subset          0.0\n",
            "support         0.0\n",
            "synthetic       0.0\n",
            "type            0.0\n",
            "used            0.0\n",
            "vector          0.0\n",
            "week            0.0\n",
            "years           0.0\n",
            "dtype: float64\n",
            "\n",
            "INITIAL RELEVANCE SCORES (COSINE SIMILARITY)\n",
            "================================================================================\n",
            "doc2: 0.7091\n",
            "doc3: 0.4497\n",
            "doc5: 0.3843\n",
            "doc6: 0.3382\n",
            "doc4: 0.2290\n",
            "doc1: 0.0000\n",
            "doc7: 0.0000\n",
            "doc8: 0.0000\n",
            "\n",
            "RELEVANT DOCUMENTS FOR QUERY EXPANSION\n",
            "================================================================================\n",
            "['doc2', 'doc3', 'doc5']\n",
            "\n",
            "IRRELEVANT DOCUMENTS FOR QUERY EXPANSION\n",
            "================================================================================\n",
            "['doc6', 'doc4', 'doc1', 'doc7', 'doc8']\n",
            "\n",
            "EXPANDED QUERY VECTOR (AFTER ROCCHIO)\n",
            "================================================================================\n",
            "1               0.000000\n",
            "100             0.000000\n",
            "5               0.000000\n",
            "algorithm       0.067967\n",
            "artificial      0.061412\n",
            "average         0.000000\n",
            "aviation        0.000000\n",
            "based           0.000000\n",
            "blends          0.000000\n",
            "chat            0.000000\n",
            "compared        0.000000\n",
            "deep            0.070491\n",
            "fell            0.000000\n",
            "fields          0.079295\n",
            "fuel            0.000000\n",
            "fuels           0.000000\n",
            "global          0.000000\n",
            "gpt             0.000000\n",
            "has             0.000000\n",
            "human           0.000000\n",
            "intelligence    0.061412\n",
            "jet             0.000000\n",
            "language        0.000000\n",
            "last            0.000000\n",
            "learning        1.202349\n",
            "life            0.000000\n",
            "machine         1.161390\n",
            "machines        0.067967\n",
            "model           0.000000\n",
            "natural         0.000000\n",
            "nearly          0.000000\n",
            "networks        0.000000\n",
            "neural          0.000000\n",
            "petroleum       0.000000\n",
            "price           0.000000\n",
            "processing      0.000000\n",
            "related         0.079295\n",
            "subset          0.095154\n",
            "support         0.067967\n",
            "synthetic       0.000000\n",
            "type            0.067967\n",
            "used            0.000000\n",
            "vector          0.067967\n",
            "week            0.000000\n",
            "years           0.000000\n",
            "dtype: float64\n",
            "\n",
            "TOP 10 TERMS IN EXPANDED QUERY\n",
            "================================================================================\n",
            "learning: 1.2023\n",
            "machine: 1.1614\n",
            "subset: 0.0952\n",
            "fields: 0.0793\n",
            "related: 0.0793\n",
            "deep: 0.0705\n",
            "machines: 0.0680\n",
            "support: 0.0680\n",
            "type: 0.0680\n",
            "algorithm: 0.0680\n",
            "\n",
            "FINAL RELEVANCE SCORES WITH ROCCHIO EXPANSION\n",
            "================================================================================\n",
            "doc2: 0.7526\n",
            "doc3: 0.5197\n",
            "doc5: 0.4632\n",
            "doc6: 0.3345\n",
            "doc4: 0.2485\n",
            "doc7: 0.0215\n",
            "doc1: 0.0000\n",
            "doc8: 0.0000\n",
            "\n",
            "COMPARISON OF RANKINGS\n",
            "================================================================================\n",
            "Original Ranking vs. Expanded Query Ranking\n",
            "Rank 1: doc2 → doc2\n",
            "Rank 2: doc3 → doc3\n",
            "Rank 3: doc5 → doc5\n",
            "Rank 4: doc6 → doc6\n",
            "Rank 5: doc4 → doc4\n",
            "Rank 6: doc1 → doc7\n",
            "Rank 7: doc7 → doc1\n",
            "Rank 8: doc8 → doc8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK9oqySu92CX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}