{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odK4wBUAt4ER",
        "outputId": "da4dae50-90bb-4b8d-8d2f-41c40a80b8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', 'bro', ',', 'Mr.', \"O'neill\", 'is', 'king', 'of', 'Denmart', ';', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '?', 'post-tagging', 'give', 'the', 'sign', 'for', 'every', 'term']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# tokenisasi kata\n",
        "import nltk # import the nltk module\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt') # download the punkt tokenizer data\n",
        "nltk.download('punkt_tab') # download the punkt_tab tokenizer data\n",
        "S = \"Hey bro, Mr. O'neill is king of Denmart; He smiled!! This, i.e. that, is it? post-tagging give the sign for every term\"\n",
        "word_token = word_tokenize(S)\n",
        "\n",
        "print(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengapa tidak menggunakan fungsi split dari python?? apa bedanya?\n",
        "\n",
        "word_split = S.split()\n",
        "print(word_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up4o2J_luG2v",
        "outputId": "1dafc994-fffa-4329-91b6-33a8a416c287"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', 'bro,', 'Mr.', \"O'neill\", 'is', 'king', 'of', 'Denmart;', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it?', 'post-tagging', 'give', 'the', 'sign', 'for', 'every', 'term']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenisasi kalimat\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "sentence_token = sent_tokenize(S)\n",
        "print(sentence_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieJTkK3auoCV",
        "outputId": "6b36e7e7-7b55-474d-c865-b173f2abb882"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Hey bro, Mr. O'neill is king of Denmart; He smiled!!\", 'This, i.e.', 'that, is it?', 'post-tagging give the sign for every term']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S7aHNBXGu2xo",
        "outputId": "fdf46d3f-beef-4e19-8c4a-5885c90c8e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/types.py\", line 25, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 367, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 216, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh tokenisasi kata menggunakan Spcay\n",
        "import spacy\n",
        "\n",
        "# Loading language model\n",
        "# spacy_en = spacy.load('en') # This line caused the error. Replaced with below\n",
        "spacy_en = spacy.load(\"en_core_web_sm\") # Load using the full model name\n",
        "\n",
        "S = \"Hey bro, Mr. O'neill is king of Denmart; He smiled!! This, i.e. that, is it? post-tagging give the sign for every term\"\n",
        "tokens = spacy_en(S)\n",
        "print( [token.text for token in tokens] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCFXmU63uqb_",
        "outputId": "f4650f21-ba01-48a1-afc7-d5d1ebd547e0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', 'bro', ',', 'Mr.', \"O'neill\", 'is', 'king', 'of', 'Denmart', ';', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '?', 'post', '-', 'tagging', 'give', 'the', 'sign', 'for', 'every', 'term']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh tokenisasi kalimat dengan Spacy\n",
        "sentence_tokens = spacy_en(S).sents\n",
        "print([str(sent) for sent in sentence_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AME3MRj-uuxz",
        "outputId": "bb98a9cd-083c-4311-b5a7-19abc8d46df8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Hey bro, Mr. O'neill is king of Denmart; He smiled!!\", 'This, i.e. that, is it?', 'post-tagging give the sign for every term']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh tokenisasi dengan TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "T = \"Hey bro, Mr. O'neill is king of Denmart; He smiled!! This, i.e. that, is it? post-tagging give the sign for every term\"\n",
        "sentence_tokens = TextBlob(T).sentences\n",
        "\n",
        "# Tokenisasi kata\n",
        "print(TextBlob(T).words)\n",
        "\n",
        "# Tokenisasi kalimat\n",
        "print([str(sent) for sent in sentence_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdsU1Ku6vGOU",
        "outputId": "0a260bd7-885d-42b1-a55c-196856b8dd9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', 'bro', 'Mr', \"O'neill\", 'is', 'king', 'of', 'Denmart', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it', 'post-tagging', 'give', 'the', 'sign', 'for', 'every', 'term']\n",
            "[\"Hey bro, Mr. O'neill is king of Denmart; He smiled!!\", 'This, i.e.', 'that, is it?', 'post-tagging give the sign for every term']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Tokenizer untuk twitter\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tweet = \"@stki I am so happpppppppy, supeeeer happpy :), aren't you? #imsohappy #happy\"\n",
        "print(Tokenizer.tokenize(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS1OZ7gyvIRa",
        "outputId": "1c639916-f45c-4811-c68c-8f774fea1e2d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'so', 'happpy', ',', 'supeeer', 'happpy', ':)', ',', \"aren't\", 'you', '?', '#imsohappy', '#happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "tweet = \"@stki I am so happpppppppppy, aren't you\"\n",
        "\n",
        "# Menghilangkan double karakter\n",
        "tweet_clear = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
        "print(tweet_clear)\n",
        "\n",
        "# NOTES: untuk beberapa data spesifik seperti data bioinformatics, cryptography,\n",
        "# twitter, dst dibutuhkan tokenizer custom untuk dapat memenuhi kebutuha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NAyen-VvKxg",
        "outputId": "842f6521-5f49-43a5-fd51-037240d38e96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@stki I am so happy, aren't you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
        "from spacy.lang.id import Indonesian\n",
        "\n",
        "# load language model bahasa Indonesia\n",
        "spacy_id = Indonesian()\n",
        "\n",
        "S = \"Hari Jum'at, Hamzah berjalan-jalan melihat kupu-kupu di taman. Ibunya membeli oleh-oleh di pasar\"\n",
        "word_token = spacy_id(S)\n",
        "print([token.text for token in word_token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEC6DBeCvNWt",
        "outputId": "b0643467-f58d-419f-c958-175ef3d4fdea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hari', \"Jum'at\", ',', 'Hamzah', 'berjalan-jalan', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibunya', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jika menggunakan Language model English:\n",
        "word_token_en = spacy_en(S)\n",
        "print([token.text for token in word_token_en])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-TmpX3jvPCE",
        "outputId": "e6dc0713-418f-4e23-d895-80dc8d141f71"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hari', \"Jum'at\", ',', 'Hamzah', 'berjalan', '-', 'jalan', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibunya', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh casefolding\n",
        "S = \"Hi there!, I am a PHD student. Nice to meet you :)\"\n",
        "print(S.lower())\n",
        "print(S.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCmQzI5svSOM",
        "outputId": "aa18ff08-dead-4325-e19d-18b1d3d6a070"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi there!, i am a phd student. nice to meet you :)\n",
            "HI THERE!, I AM A PHD STUDENT. NICE TO MEET YOU :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Stemming di NLTK\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "S = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
        "print('Sentence: ',S)\n",
        "\n",
        "stemmer_list = [LancasterStemmer, PorterStemmer, SnowballStemmer]\n",
        "names = ['Lancaster', 'Porter', 'SnowBall']\n",
        "for stemmer_name,stem in zip(names,stemmer_list):\n",
        "    if stemmer_name == 'SnowBall':\n",
        "        st = stem('english')\n",
        "    else:\n",
        "        st = stem()\n",
        "    print(stemmer_name,': ',' '.join(st.stem(s) for s in S.split()))\n",
        "# perhatikan, kita tidak melakukan case normalization (lowercase)\n",
        "# Hasil stemming bisa tidak bermakna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH0qPExSvUKJ",
        "outputId": "f6233c4a-5dff-4ba7-f88d-1b0ae3ddf40a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  presumably I would like to MultiPly my provision, saying tHat without crYing\n",
            "Lancaster :  presum i would lik to multiply my provision, say that without cry\n",
            "Porter :  presum i would like to multipli my provision, say that without cri\n",
            "SnowBall :  presum i would like to multipli my provision, say that without cri\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Lemmatizer di NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "S = \"apples and Oranges are similar. boots and hippos aren't, don't you think?\"\n",
        "print('Sentence: ', S)\n",
        "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(s) for s in S.split()))\n",
        "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8lKowHtvXBs",
        "outputId": "5d380640-bca8-4dca-94eb-08285c803c11"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  apples and Oranges are similar. boots and hippos aren't, don't you think?\n",
            "Lemmatize:  apple and Oranges are similar. boot and hippo aren't, don't you think?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukm5R2vMvZMS",
        "outputId": "f10e3dde-9fe8-48b9-8ed1-f6ee8c093868"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh TextBlob Stemming & Lemmatizer\n",
        "from textblob import Word\n",
        "# Stemming\n",
        "print(\"Stem: \", Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
        "\n",
        "# Lemmatizer\n",
        "print(\"Lemmatize: \", Word('went').lemmatize('v'))\n",
        "\n",
        "# default Noun, plural akan menjadi singular dari akar katanya\n",
        "# Juga case sensitive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WLvUf6_vruc",
        "outputId": "44f56610-8619-4442-e7da-9b2ee1a1debf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem:  run\n",
            "Lemmatize:  go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy Lemmatizer English\n",
        "sent = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
        "sent_token = spacy_en(sent)\n",
        "print( ' '.join( s.lemma_ for s in sent_token ) )\n",
        "# HATI-HATI dengan lemma \"I\" dan \"you\" di Spacy!!!..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTm1SY2Ivtmt",
        "outputId": "f315ca59-49cd-4b36-afaf-3b89f9428ef2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I be sure apple and orange be similar . boot and hippos be not , do not you think ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy Lemmatizer Indonesia\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Jogjakarta\"\n",
        "idn = spacy_id(I)\n",
        "print( ' '.join( k.lemma_ for k in idn ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7q3mjHXvvOR",
        "outputId": "84a9306c-16e3-4c9a-fb0a-e1b38d97d21c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perhatikan output berikut (hati-hati inkonsistensi)\n",
        "print([k.lemma_ for k in spacy_id(\"Perayaan Bepergian\")])\n",
        "\n",
        "# bagaimana dengan Spacy stemmer??\n",
        "# Spacy belum support stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyn-dFw0vwx2",
        "outputId": "6b74ece9-e0b4-49c8-b07b-bd6b14b9c38a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qriUTXKcv3K4",
        "outputId": "f1575b83-d9b1-4e48-dc05-1b1fa964af21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl.metadata (892 bytes)\n",
            "Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/210.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer dengan Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Makassar\"\n",
        "print(stemmer.stem(I))\n",
        "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
        "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
        "# Apa sajakah?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cskfKN8Gvyh4",
        "outputId": "fa77d896-d5a9-4692-abf2-be98359d87ce"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raya itu bareng dengan saat kita pergi ke makassar\n",
            "raya pergi suara\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh POS tags dengan NLTK (bahasa Inggris)\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "S = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
        "\n",
        "tokens = word_tokenize(S)\n",
        "print(pos_tag(tokens))\n",
        "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-zwVxNDv0kC",
        "outputId": "cb7840ab-b7a4-430c-d690-80518ff5564e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh POS tag dengan TextBlob pada bahasa Inggris\n",
        "for word, pos in TextBlob(T).tags:\n",
        "    print(word, pos, end=', ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_HAyoZpv8sD",
        "outputId": "988e82e2-0036-4fdc-a424-0f15f394183f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey NNP, bro NN, Mr. NNP, O'neill NNP, is VBZ, king VBG, of IN, Denmart NNP, He PRP, smiled VBD, This DT, i.e NN, that DT, is VBZ, it PRP, post-tagging JJ, give VBP, the DT, sign NN, for IN, every DT, term NN, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh POS tag dengan Spacy pada bahasa Inggris\n",
        "tokens = spacy_en(T)\n",
        "for token in tokens:\n",
        "    print(token,token.tag_, end =', ')\n",
        "\n",
        "# Spacy belum support POS tag untuk bahasa Indonesia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWFVh6sQwC2u",
        "outputId": "8f2cbe7c-c2af-467b-c24f-f595e96b7105"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey UH, bro NN, , ,, Mr. NNP, O'neill NNP, is VBZ, king NN, of IN, Denmart NNP, ; :, He PRP, smiled VBD, ! ., ! ., This DT, , ,, i.e. FW, that IN, , ,, is VBZ, it PRP, ? ., post JJ, - JJ, tagging JJ, give VB, the DT, sign NN, for IN, every DT, term NN, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk mengetahui arti dari POS tag pada Spacy dapat menggunakan perintah \"explain\"\n",
        "spacy.explain('RB')\n",
        "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "azadFYJWwEf_",
        "outputId": "e815de77-326c-4024-c7a7-4a3768d7cc58"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adverb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB6UARZUwlPx",
        "outputId": "08d002ec-ad78-43a3-da1d-ed4636a0d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tag bahasa Indonesia dengan NLTK\n",
        "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
        "from nltk.tag import CRFTagger\n",
        "ct = CRFTagger()\n",
        "ct.set_model_file('/content/all_indo_man_tag_corpus_model.crf.tagger')\n",
        "\n",
        "hasil = ct.tag_sents([['Saya','bekerja','di','Bandung']])\n",
        "print(hasil)\n",
        "# Hati-hati dengan struktur data inputnya"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g4gMA8twGgW",
        "outputId": "294aab1e-47e2-4985-dc65-cebb9ad12f05"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Saya', 'PRP'), ('bekerja', 'VB'), ('di', 'IN'), ('Bandung', 'NNP')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh stopword dari NLTK\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk_stw_en = stopwords.words('english')\n",
        "print(nltk_stw_en[:10])\n",
        "\n",
        "nltk_stw_id = stopwords.words('indonesian')\n",
        "print(nltk_stw_id[:10])\n",
        "\n",
        "# Lsit stopword dapat ditambahkan dan dikurangi sesuai dengan kebutuhan"
      ],
      "metadata": {
        "id": "poOeZa4KwIJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e856a73-8010-41a6-b345-952950b9217f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#contoh stopword dari Sastrawi pada bahasa Indonesia\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "sastrawi_stw_id = factory.get_stop_words()\n",
        "print(sastrawi_stw_id[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMcBe__ozSta",
        "outputId": "cb169ace-0014-48bc-963b-167d3e0355cc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tips:\n",
        "# selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
        "nltk_stw_en = set(nltk_stw_en)\n",
        "nltk_stw_id = set(nltk_stw_id)\n",
        "sastrawi_stw_id = set(sastrawi_stw_id)"
      ],
      "metadata": {
        "id": "125CAviyzXJn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yz4TO10ezZaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}